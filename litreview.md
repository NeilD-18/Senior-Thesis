## Hendrycks & Dietterich (2019) – Benchmarking Neural Network Robustness To Common Corruptions and Perturbations

Hendrycks and Dietterich (2019) argue that robustness in machine learning should not only be studied through adversarial examples but also through more common and naturally occurring data corruptions. In this context, an adversarial example is one that is intentionally chosen to fool the machine learning model. For example, take an image of a panda and add some specifc noise to the pixels. This might be random noise to humans, but to the model it could classify the panda as an alligator (or something else incorrect) with 99% confidence. To address this gap, they introduce two standardized evaluation benchmarks for image classification. IMAGENET-C applies 75 algorithmic corruptions (e.g., noise, blur, weather, and digital distortions) at multiple severity levels, while IMAGENET-P measures model stability under small changes over time such as translations and rotations. They also propose new metrics, including the mean Corruption Error (mCE) and mean Flip Rate (mFR), to quantify robustness in a reproducible way.

Their empirical results show that while accuracy on clean images improved substantially from AlexNet to ResNet, robustness to corruptions and perturbations did not improve at the same rate. For some context on these neural network architectures, AlexNet (2012) was the first deep convolutional neural network to win the ImageNet competition, using 8 layers and GPU training to dramatically improve image classification accuracy. ResNet (2015) is a much deeper network (50+ layers) that introduced residual/skip connections, allowing very deep models to train effectively and achieve state-of-the-art performance. Neural networks often exhibited unstable predictions under small, natural perturbations, even when these perturbations would not challenge human vision. Some methods, such as histogram equalization preprocessing, multiscale architectures, and larger feature-aggregation models (DenseNets, ResNeXts), demonstrated moderate robustness gains. Interestingly, certain adversarial training techniques also improved robustness to common corruptions, suggesting connections between these two research areas.

This work is important because it redefined robustness evaluation as a benchmarkable property of models rather than an ad hoc test, providing a standardized way to measure progress. The authors emphasize that future improvements in machine learning should be assessed not just by clean accuracy but also by corruption robustness, since real-world data is rarely pristine.  

In my project, this framework serves as a direct inspiration. Although Hendrycks and Dietterich focus on image classification, my work extends the spirit of their evaluation to structured tabular and text data. I plan to apply systematic corruptions such as missing values, noisy features, and distributional shifts to benchmark how algorithms (e.g., decision trees, random forests, gradient boosting, neural networks) degrade in performance. Their findings caution that architectural improvements alone do not guarantee robustness, which is an important consideration for my evaluation of classical ML models under imperfect data conditions.


## Rolnick et al. (2017) – Deep Learning is Robust to Massive Label Noise

Rolnick, Veit, Belongie, and Shavit (2017) examine how deep neural networks perform when trained on datasets containing extreme levels of label noise. Label noise occurs when the assigned class labels are incorrect, either randomly or systematically, and has traditionally been viewed as highly detrimental to model performance. The authors challenge this assumption by showing that standard deep learning models can still generalize well, even when the vast majority of training labels are wrong.

Using MNIST, CIFAR-10, and ImageNet, they demonstrate that convolutional and residual networks retain high test accuracy despite massive noise. On MNIST, for example, networks maintained over 90% test accuracy when each correctly labeled example was paired with 100 randomly mislabeled ones. Similarly, on CIFAR-10, models achieved roughly 85% accuracy with 10 noisy labels per clean label, and on ImageNet, about 70% top-5 accuracy with 5 noisy labels per clean one. These results suggest that deep models are not simply memorizing noise, but are instead leveraging the clean portion of the data to learn meaningful patterns.

The paper notes this robustess partly to dataset scale and gradient averaging. The authors also distinguish between random label noise and adversarial noise, noting that deep networks are far more resilient to random mislabeling than to deliberately crafted perturbations.

This work is important because it challenges the idea that neural networks are fragile, showing that having enough data can offset the impact of incorrect labels. In my project, this insight motivates experiments that intentionally inject label noise into structured and textual datasets—such as the Adult Income and IMDB sentiment datasets—to measure how classical machine learning models degrade under similar conditions. Like Hendrycks and Dietterich (2019), Rolnick et al. emphasize that robustness should be measured empirically, and their findings provide strong evidence that some architectures can maintain reliable performance even when supervision is unreliable.


## Szegedy et al. (2014) – Intriguing Properties of Neural Networks
Szegedy, Zaremba, Sutskever, Bruna, Erhan, Goodfellow, and Fergus (2014) were among the first to uncover that deep neural networks, despite their impressive accuracy, are surprisingly fragile. They showed that small, carefully chosen perturbations to input data, invisible to humans, can cause a model to confidently misclassify an image. These inputs are known as adversarial examples.

Their study found that adversarial examples are not just isolated to one model, they transfer across architectures. A test designed to fool one neural network often fools others trained on the same dataset, suggesting that these vulnerabilities stem from shared patterns in how models learn decision boundaries. This finding challenged the assumption that high test accuracy implies reliability, highlighting that deep models can behave unpredictably under small, non-random shifts.

This work is important because it redefined how researchers think about robustness. It showed that neural networks are not simply overfitting, but are sensitive to certain structured changes in input space. The authors also discussed how model sensitivity might relate to properties like layer scaling or optimization dynamics, leading to the rise of adversarial training and other robustness techniques.

For my project, this paper motivates exploring how small, systematic changes in structured or textual data—like slightly shifting numerical features or replacing sentiment-bearing words—can alter model predictions. While Szegedy et al. focused on image classifiers, their work establishes the principle that high-performing models can still fail dramatically when data deviates in subtle but meaningful ways. This connects directly to my goal of studying how classical machine learning models degrade under noisy or shifted data conditions.


##  Taori et al. (2020) – Measuring Robustness to Natural Distribution Shifts in Image Classification

Taori, Dave, Shankar, Carlini, Recht, and Schmidt (2020) investigate how well modern image classification models maintain performance when evaluated on data drawn from the same underlying distribution but collected under slightly different conditions. This is known as natural distribution shift. Unlike adversarial perturbations, these shifts occur naturally over time or across different data sources. For example, a model trained on ImageNet images might perform worse on a newer, independently collected ImageNet-style dataset due to subtle differences in lighting, background, or object framing.

To study this, the authors introduce ImageNet-V2, a new test set that mirrors the original ImageNet validation set but was created seperately using the same class labels and data collection procedures. By comparing performance on ImageNet and ImageNet-V2, they measure how much model accuracy drops under natural variations in data. Across a wide range of architectures, such as ResNets, DenseNets, and EfficientNets, they find a consistent drop of 11–14 percentage points in top-1 accuracy. Importantly, this degradation was not specific to any one model family, suggesting that improved architectures and larger datasets alone do not ensure robustness to real-world shifts.

The paper also examines whether relative model rankings remain consistent across datasets. They find that models performing better on the original ImageNet also tend to perform better on ImageNet-V2, but the absolute gap in accuracy remains substantial. This means progress measured by traditional benchmarks may overstate how well models generalize beyond their training environment.

This work is important because it reframes robustness as a question of generalization under natural data shifts, not just adversarial attacks or artificial noise. In my project, this concept directly relates to evaluating classical machine learning models under distributional shifts, for instance, training on one dataset (such as IMDB) and testing on another with different domain characteristics (such as Amazon reviews). Similar to Taori et al., my experiments aim to quantify how performance changes when test data differ subtly but systematically from the training distribution, helping assess true model robustness in more realistic settings.


## Wang et al. (2022) – Generalizing to Unseen Domains: A Survey on Domain Generalization

Wang, Lan, Liu, Ouyang, Qin, Lu, Chen, Zeng, and Yu (2022) provide a comprehensive overview of domain generalization (DG), which is the challenge of building models that perform well on unseen test domains where data distributions differ from training data. While traditional machine learning assumes that training and testing data are drawn from the same underlying distribution, this assumption rarely holds in real-world scenarios. Small changes in lighting, demographics, equipment, or language can shift data distributions enough to degrade model performance. Domain generalization seeks to address this by training models to extract features that remain stable across diverse domains, enabling them to generalize to new, unseen conditions without direct exposure to that data during training.

The authors distinguish domain generalization from related concepts such as domain adaptation and transfer learning. Unlike domain adaptation, which assumes some access to the target domain during training, domain generalization operates under a stricter setting, where no target domain data are available. To address this, Wang et al. categorize DG methods into three main strategies. The first, data manipulation, expands training diversity through data augmentation, style transfer, or synthetic domain generation. The second, representation learning, aims to learn domain-invariant features that capture essential relationships while ignoring domain-specific noise, often through adversarial or contrastive objectives. The third, learning strategies, involves modifying the training process itself — for example, using meta-learning frameworks that simulate domain shifts by holding out subsets of data during training to mimic unseen test domains.

The paper also discusses emerging ideas that connect domain generalization with causal inference and optimization theory. For example, methods like Invariant Risk Minimization (IRM) and causality-based feature disentanglement attempt to identify the underlying mechanisms that remain consistent across environments, rather than simply optimizing for average performance.

This work is important because it reframes robustness as a broader problem of generalizing across real-world distribution shifts, rather than resisting small, artificial perturbations. In my project, this perspective directly informs how I evaluate model robustness under structured and textual data shifts. Just as domain generalization measures performance when test data differ subtly but systematically from training data, my experiments test how classical machine learning models (e.g., decision trees, random forests, neural networks) degrade when exposed to new distributions, missing values, or noisy features. 
