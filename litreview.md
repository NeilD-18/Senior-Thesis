### Hendrycks & Dietterich (2019) – Benchmarking Neural Network Robustness To Common Corruptions and Perturbations

Hendrycks and Dietterich (2019) argue that robustness in machine learning should not only be studied through adversarial examples but also through more common and naturally occurring data corruptions. In this context, an adversarial example is one that is intentionally chosen to fool the machine learning model. For example, take an image of a panda and add some specifc noise to the pixels. This might be random noise to humans, but to the model it could classify the panda as an alligator (or something else incorrect) with 99% confidence. To address this gap, they introduce two standardized evaluation benchmarks for image classification. IMAGENET-C applies 75 algorithmic corruptions (e.g., noise, blur, weather, and digital distortions) at multiple severity levels, while IMAGENET-P measures model stability under small changes over time such as translations and rotations. They also propose new metrics, including the mean Corruption Error (mCE) and mean Flip Rate (mFR), to quantify robustness in a reproducible way.

Their empirical results show that while accuracy on clean images improved substantially from AlexNet to ResNet, robustness to corruptions and perturbations did not improve at the same rate. For some context on these neural network architectures, AlexNet (2012) was the first deep convolutional neural network to win the ImageNet competition, using 8 layers and GPU training to dramatically improve image classification accuracy. ResNet (2015) is a much deeper network (50+ layers) that introduced residual/skip connections, allowing very deep models to train effectively and achieve state-of-the-art performance. Neural networks often exhibited unstable predictions under small, natural perturbations, even when these perturbations would not challenge human vision. Some methods, such as histogram equalization preprocessing, multiscale architectures, and larger feature-aggregation models (DenseNets, ResNeXts), demonstrated moderate robustness gains. Interestingly, certain adversarial training techniques also improved robustness to common corruptions, suggesting connections between these two research areas.

This work is important because it redefined robustness evaluation as a benchmarkable property of models rather than an ad hoc test, providing a standardized way to measure progress. The authors emphasize that future improvements in machine learning should be assessed not just by clean accuracy but also by corruption robustness, since real-world data is rarely pristine.  

In my project, this framework serves as a direct inspiration. Although Hendrycks and Dietterich focus on image classification, my work extends the spirit of their evaluation to structured tabular and text data. I plan to apply systematic corruptions such as missing values, noisy features, and distributional shifts to benchmark how algorithms (e.g., decision trees, random forests, gradient boosting, neural networks) degrade in performance. Their findings caution that architectural improvements alone do not guarantee robustness, which is an important consideration for my evaluation of classical ML models under imperfect data conditions.


### Rolnick et al. (2017) – Deep Learning is Robust to Massive Label Noise

Rolnick, Veit, Belongie, and Shavit (2017) examine how deep neural networks perform when trained on datasets containing extreme levels of label noise. Label noise occurs when the assigned class labels are incorrect, either randomly or systematically, and has traditionally been viewed as highly detrimental to model performance. The authors challenge this assumption by showing that standard deep learning models can still generalize well, even when the vast majority of training labels are wrong.

Using MNIST, CIFAR-10, and ImageNet, they demonstrate that convolutional and residual networks retain high test accuracy despite massive noise. On MNIST, for example, networks maintained over 90% test accuracy when each correctly labeled example was paired with 100 randomly mislabeled ones. Similarly, on CIFAR-10, models achieved roughly 85% accuracy with 10 noisy labels per clean label, and on ImageNet, about 70% top-5 accuracy with 5 noisy labels per clean one. These results suggest that deep models are not simply memorizing noise, but are instead leveraging the clean portion of the data to learn meaningful patterns.

The paper notes this robustess partly to dataset scale and gradient averaging. The authors also distinguish between random label noise and adversarial noise, noting that deep networks are far more resilient to random mislabeling than to deliberately crafted perturbations.

This work is important because it challenges the idea that neural networks are fragile, showing that having enough data can offset the impact of incorrect labels. In my project, this insight motivates experiments that intentionally inject label noise into structured and textual datasets—such as the Adult Income and IMDB sentiment datasets—to measure how classical machine learning models degrade under similar conditions. Like Hendrycks and Dietterich (2019), Rolnick et al. emphasize that robustness should be measured empirically, and their findings provide strong evidence that some architectures can maintain reliable performance even when supervision is unreliable.


### Szegedy et al. (2014) – Intriguing Properties of Neural Networks
Szegedy, Zaremba, Sutskever, Bruna, Erhan, Goodfellow, and Fergus (2014) were among the first to uncover that deep neural networks, despite their impressive accuracy, are surprisingly fragile. They showed that small, carefully chosen perturbations to input data—imperceptible to humans—can cause a model to confidently misclassify an image. These inputs are known as adversarial examples.

Their study found that adversarial examples are not just isolated to one model, they transfer across architectures. A test designed to fool one neural network often fools others trained on the same dataset, suggesting that these vulnerabilities stem from shared patterns in how models learn decision boundaries. This finding challenged the assumption that high test accuracy implies reliability, highlighting that deep models can behave unpredictably under small, non-random shifts.

This work is important because it redefined how researchers think about robustness. It showed that neural networks are not simply overfitting, but are sensitive to certain structured changes in input space. The authors also discussed how model sensitivity might relate to properties like layer scaling or optimization dynamics, leading to the rise of adversarial training and other robustness techniques.

For my project, this paper motivates exploring how small, systematic changes in structured or textual data—like slightly shifting numerical features or replacing sentiment-bearing words—can alter model predictions. While Szegedy et al. focused on image classifiers, their work establishes the principle that high-performing models can still fail dramatically when data deviates in subtle but meaningful ways. This connects directly to my goal of studying how classical machine learning models degrade under noisy or shifted data conditions.
