### Hendrycks & Dietterich (2019) â€“ Benchmarking Neural Network Robustness To Common Corruptions and Perturbations

Hendrycks and Dietterich (2019) argue that robustness in machine learning should not only be studied through adversarial examples but also through more common and naturally occurring data corruptions. In this context, an adversarial example is one that is intentionally chosen to fool the machine learning model. For example, take an image of a panda and add some specifc noise to the pixels. This might be random noise to humans, but to the model it could classify the panda as an alligator (or something else incorrect) with 99% confidence. To address this gap, they introduce two standardized evaluation benchmarks for image classification. IMAGENET-C applies 75 algorithmic corruptions (e.g., noise, blur, weather, and digital distortions) at multiple severity levels, while IMAGENET-P measures model stability under small changes over time such as translations and rotations. They also propose new metrics, including the mean Corruption Error (mCE) and mean Flip Rate (mFR), to quantify robustness in a reproducible way.

Their empirical results show that while accuracy on clean images improved substantially from AlexNet to ResNet, robustness to corruptions and perturbations did not improve at the same rate. For some context on these neural network architectures, AlexNet (2012) was the first deep convolutional neural network to win the ImageNet competition, using 8 layers and GPU training to dramatically improve image classification accuracy. ResNet (2015) is a much deeper network (50+ layers) that introduced residual/skip connections, allowing very deep models to train effectively and achieve state-of-the-art performance. Neural networks often exhibited unstable predictions under small, natural perturbations, even when these perturbations would not challenge human vision. Some methods, such as histogram equalization preprocessing, multiscale architectures, and larger feature-aggregation models (DenseNets, ResNeXts), demonstrated moderate robustness gains. Interestingly, certain adversarial training techniques also improved robustness to common corruptions, suggesting connections between these two research areas.

This work is important because it redefined robustness evaluation as a benchmarkable property of models rather than an ad hoc test, providing a standardized way to measure progress. The authors emphasize that future improvements in machine learning should be assessed not just by clean accuracy but also by corruption robustness, since real-world data is rarely pristine.  

In my project, this framework serves as a direct inspiration. Although Hendrycks and Dietterich focus on image classification, my work extends the spirit of their evaluation to structured tabular and text data. I plan to apply systematic corruptions such as missing values, noisy features, and distributional shifts to benchmark how algorithms (e.g., decision trees, random forests, gradient boosting, neural networks) degrade in performance. Their findings caution that architectural improvements alone do not guarantee robustness, which is an important consideration for my evaluation of classical ML models under imperfect data conditions.
