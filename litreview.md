### Hendrycks & Dietterich (2019) â€“ Benchmarking Neural Network Robustness To Common Corruptions and Perturbations

Hendrycks and Dietterich (2019) argue that robustness in machine learning should not only be studied through adversarial examples but also through more common and naturally occurring data corruptions. In this context, an adversarial example is one that is intentionally chosen to fool the machine learning model. For example, take an image of a panda and add some specifc noise to the pixels. This might be random noise to humans, but to the model it could classify the panda as an alligator (or something else incorrect) with 99% confidence. To address this gap, they introduce two standardized evaluation benchmarks for image classification: **IMAGENET-C**, which applies 75 algorithmic corruptions (noise, blur, weather, and digital distortions) at multiple severity levels, and **IMAGENET-P**, which measures model stability under sequential perturbations such as translations and rotations. They further propose metrics such as the mean Corruption Error (mCE) and mean Flip Rate (mFR) to quantify robustness in a reproducible way.

Their empirical results show that while accuracy on clean images improved substantially from AlexNet to ResNet, robustness to corruptions and perturbations did not improve at the same rate. Neural networks often exhibited unstable predictions under small, natural perturbations, even when these perturbations would not challenge human vision. Some methods, such as histogram equalization preprocessing, multiscale architectures, and larger feature-aggregation models (DenseNets, ResNeXts), demonstrated moderate robustness gains. Interestingly, certain adversarial training techniques also improved robustness to common corruptions, suggesting connections between these two research areas.

This work is important because it redefined robustness evaluation as a benchmarkable property of models rather than an ad hoc test, providing a standardized way to measure progress. The authors emphasize that future improvements in machine learning should be assessed not just by clean accuracy but also by corruption robustness, since real-world data is rarely pristine.  

In my project, this framework serves as a direct inspiration. Although Hendrycks and Dietterich focus on image classification, my work extends the spirit of their evaluation to structured tabular and text data. I plan to apply systematic corruptions such as missing values, noisy features, and distributional shifts to benchmark how algorithms (e.g., decision trees, random forests, gradient boosting, neural networks) degrade in performance. Their findings caution that architectural improvements alone do not guarantee robustness, which is an important consideration for my evaluation of classical ML models under imperfect data conditions.
